
@inproceedings{cartucho_robust_2018,
	title = {Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots},
	doi = {10.1109/IROS.2018.8594067},
	abstract = {Despite the recent success of state-of-the-art deep learning algorithms in object recognition, when these are deployed as-is on a mobile service robot, we observed that they failed to recognize many objects in real human environments. In this paper, we introduce a learning algorithm in which robots address this flaw by asking humans for help, also known as a symbiotic autonomy approach. In particular, we bootstrap {YOLOv}2, a state-of-the-art deep neural network and train a new neural network, that we call {HHELP}, using only data collected from human help. Using an {RGB} camera and an onboard tablet, the robot proactively seeks human input to assist it in labeling surrounding objects. Pepper, located at {CMU}, and Monarch Mbot, located at {ISR}-Lisbon, were the service robots that we used to validate the proposed approach. We conducted a study in a realistic domestic environment over the course of 20 days with 6 research participants. To improve object detection, we used the two neural networks, {YOLOv}2 + {HHELP}, in parallel. Following this methodology, the robot was able to detect twice the number of objects compared to the initial {YOLOv}2 neural network, and achieved a higher {mAP} (mean Average Precision) score. Using the learning algorithm the robot also collected data about where an object was located and to whom it belonged to by asking humans. This enabled us to explore a future use case where robots can search for a specific person's object. We view the contribution of this work to be relevant for service robots in general, in addition to Pepper, and Mbot.},
	pages = {2336--2341},
	booktitle = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	author = {Cartucho, João and Ventura, Rodrigo and Veloso, Manuela},
	date = {2018-10},
	note = {{ISSN}: 2153-0866},
}

@software{cartucho_cartuchomap_2021,
	title = {Cartucho/{mAP}},
	rights = {Apache-2.0},
	url = {https://github.com/Cartucho/mAP},
	abstract = {mean Average Precision - This code evaluates the performance of your neural net for object recognition.},
	author = {Cartucho},
	urldate = {2021-06-28},
	date = {2021-06-27},
	note = {original-date: 2018-03-08T11:24:23Z},
	keywords = {average-precision, computer-vision, darkflow, darknet, detection, ground-truth, machine-learning, metrics, neural-network, object-detection, pascal-voc, yolo},
}

@article{everingham_pascal_2015,
	title = {The Pascal Visual Object Classes Challenge: A Retrospective},
	volume = {111},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-014-0733-5},
	doi = {10.1007/s11263-014-0733-5},
	abstract = {The Pascal Visual Object Classes ({VOC}) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the {VOC} datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the {VOC} datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community’s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges.},
	pages = {98--136},
	number = {1},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Eslami, S. M. Ali and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	date = {2015-01-01},
}

@article{padilla_comparative_2021,
	title = {A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit},
	volume = {10},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/10/3/279},
	doi = {10.3390/electronics10030279},
	abstract = {Recent outstanding results of supervised object detection in competitions and challenges are often associated with specific metrics and datasets. The evaluation of such methods applied in different contexts have increased the demand for annotated datasets. Annotation tools represent the location and size of objects in distinct formats, leading to a lack of consensus on the representation. Such a scenario often complicates the comparison of object detection methods. This work alleviates this problem along the following lines: (i) It provides an overview of the most relevant evaluation methods used in object detection competitions, highlighting their peculiarities, differences, and advantages; (ii) it examines the most used annotation formats, showing how different implementations may influence the assessment results; and (iii) it provides a novel open-source toolkit supporting different annotation formats and 15 performance metrics, making it easy for researchers to evaluate the performance of their detection algorithms in most known datasets. In addition, this work proposes a new metric, also included in the toolkit, for evaluating object detection in videos that is based on the spatio-temporal overlap between the ground-truth and detected bounding boxes.},
	number = {3},
	journaltitle = {Electronics},
	author = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto, Sergio L. and da Silva, Eduardo A. B.},
	date = {2021},
}

@inproceedings{padilla_survey_2020,
	title = {A Survey on Performance Metrics for Object-Detection Algorithms},
	doi = {10.1109/IWSSIP48289.2020.9145130},
	abstract = {This work explores and compares the plethora of metrics for the performance evaluation of object-detection algorithms. Average precision ({AP}),for instance, is a popular metric for evaluating the accuracy of object detectors by estimating the area under the curve ({AUC}) of the precision × recall relationship. Depending on the point interpolation used in the plot, two different {AP} variants can be defined and, therefore, different results are generated. {AP} has six additional variants increasing the possibilities of benchmarking. The lack of consensus in different works and {AP} implementations is a problem faced by the academic and scientific communities. Metric implementations written in different computational languages and platforms are usually distributed with corresponding datasets sharing a given bounding-box description. Such projects indeed help the community with evaluation tools, but demand extra work to be adapted for other datasets and bounding-box formats. This work reviews the most used metrics for object detection detaching their differences, applications, and main concepts. It also proposes a standard implementation that can be used as a benchmark among different datasets with minimum adaptation on the annotation files.},
	pages = {237--242},
	booktitle = {2020 International Conference on Systems, Signals and Image Processing ({IWSSIP})},
	author = {Padilla, Rafael and Netto, Sergio L. and da Silva, Eduardo A. B.},
	date = {2020-07},
	note = {{ISSN}: 2157-8702},
}

@article{lin_motion_2013,
	title = {Motion Vector Coding in the {HEVC} Standard},
	volume = {7},
	issn = {1941-0484},
	doi = {10.1109/JSTSP.2013.2271975},
	abstract = {High Efficiency Video Coding ({HEVC}) is an emerging international video coding standard developed by the Joint Collaborative Team on Video Coding ({JCT}-{VC}). Compared to H.264/{AVC}, {HEVC} has achieved substantial compression performance improvement. During the {HEVC} standardization, we proposed several motion vector coding techniques, which were crosschecked by other experts and then adopted into the standard. In this paper, an overview of the motion vector coding techniques in {HEVC} is firstly provided. Next, the proposed motion vector coding techniques including a priority-based derivation algorithm for spatial motion candidates, a priority-based derivation algorithm for temporal motion candidates, a surrounding-based candidate list, and a parallel derivation of the candidate list, are also presented. Based on {HEVC} test model 9 ({HM}9), experimental results show that the combination of the proposed techniques achieves on average 3.1\% bit-rate saving under the common test conditions used for {HEVC} development.},
	pages = {957--968},
	number = {6},
	journaltitle = {{IEEE} Journal of Selected Topics in Signal Processing},
	author = {Lin, Jian-Liang and Chen, Yi-Wen and Huang, Yu-Wen and Lei, Shaw-Min},
	date = {2013-12},
}

@software{labbe_rlabbefilterpy_2021,
	title = {rlabbe/filterpy},
	rights = {{MIT}},
	url = {https://github.com/rlabbe/filterpy},
	abstract = {Python Kalman filtering and optimal estimation library. Implements Kalman filter, particle filter, Extended Kalman filter, Unscented Kalman filter, g-h (alpha-beta), least squares, H Infinity, smoothers, and more. Has companion book 'Kalman and Bayesian Filters in Python'.},
	author = {Labbe, Roger},
	urldate = {2021-06-24},
	date = {2021-06-24},
	note = {original-date: 2014-07-15T02:15:19Z},
}

@article{kuhn_hungarian_1955,
	title = {The Hungarian method for the assignment problem},
	volume = {2},
	issn = {00281441},
	pages = {83--97},
	number = {1},
	journaltitle = {Naval research logistics quarterly.},
	author = {Kuhn, H. W. and Kuhn, H W},
	date = {1955},
	note = {Place: Arlington, Va. : Washington, D.C. :
Publisher: Office of Naval Research},
}

@article{kalman_new_1960,
	title = {A new approach to linear filtering and prediction problems},
	volume = {82},
	pages = {35--45},
	number = {1},
	journaltitle = {Journal of basic Engineering},
	author = {Kalman, Rudolph Emil and {Others}},
	date = {1960},
	keywords = {filtering kalman},
}

@inproceedings{li_multiple_2010,
	title = {A multiple object tracking method using Kalman filter},
	doi = {10.1109/ICINFA.2010.5512258},
	abstract = {It is important to maintain the identity of multiple targets while tracking them in some applications such as behavior understanding. However, unsatisfying tracking results may be produced due to different real-time conditions. These conditions include: inter-object occlusion, occlusion of the ocjects by background obstacles, splits and merges, which are observed when objects are being tracked in real-time. In this paper, an algorithm of feature-based using Kalman filter motion to handle multiple objects tracking is proposed. The system is fully automatic and requires no manual input of any kind for initialization of tracking. Through establishing Kalman filter motion model with the features centroid and area of moving objects in a single fixed camera monitoring scene, using information obtained by detection to judge whether merge or split occurred, the calculation of the cost function can be used to solve the problems of correspondence after split happened. The algorithm proposed is validated on human and vehicle image sequence. The results shows that the algorithm proposed achieves efficient tracking of multiple moving objects under the confusing situations.},
	eventtitle = {The 2010 {IEEE} International Conference on Information and Automation},
	pages = {1862--1866},
	booktitle = {The 2010 {IEEE} International Conference on Information and Automation},
	author = {Li, Xin and Wang, Kejun and Wang, Wei and Li, Yang},
	date = {2010-06},
	keywords = {Cameras, Cost function, Humans, Image sequences, Kalam filter, Layout, Monitoring, Motion detection, Object detection, Occlusion, Target tracking, Vehicles, motion model, multi-object tracking},
}

@inproceedings{lin_focal_2017,
	title = {Focal Loss for Dense Object Detection},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
	pages = {2980--2988},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
	urldate = {2021-05-18},
	date = {2017},
}

@inproceedings{liu_ssd_2016,
	location = {Cham},
	title = {{SSD}: Single Shot {MultiBox} Detector},
	isbn = {978-3-319-46448-0},
	doi = {10.1007/978-3-319-46448-0_2},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{SSD}},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named {SSD}, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. {SSD} is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes {SSD} easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the {PASCAL} {VOC}, {COCO}, and {ILSVRC} datasets confirm that {SSD} has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300300×300300 {\textbackslash}times 300 input, {SSD} achieves 74.3 \% {mAP} on {VOC}2007 test at 59 {FPS} on a Nvidia Titan X and for 512×512512×512512 {\textbackslash}times 512 input, {SSD} achieves 76.9 \% {mAP}, outperforming a comparable state of the art Faster R-{CNN} model. Compared to other single stage methods, {SSD} has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
	pages = {21--37},
	booktitle = {Computer Vision – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	date = {2016},
	langid = {english},
	keywords = {Convolutional neural network, Real-time object detection},
}

@inproceedings{he_mask_2017,
	title = {Mask R-{CNN}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/He_Mask_R-CNN_ICCV_2017_paper.html},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
	pages = {2961--2969},
	author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
	urldate = {2021-05-18},
	date = {2017},
}

@inproceedings{lin_feature_2017,
	title = {Feature Pyramid Networks for Object Detection},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {2117--2125},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	urldate = {2021-05-18},
	date = {2017},
}

@article{ren_faster_2017,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	volume = {39},
	doi = {10.1109/TPAMI.2016.2577031},
	pages = {1137--1149},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	date = {2017},
}

@inproceedings{girshick_fast_2015,
	title = {Fast R-{CNN}},
	url = {https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html},
	eventtitle = {Proceedings of the {IEEE} International Conference on Computer Vision},
	pages = {1440--1448},
	author = {Girshick, Ross},
	urldate = {2021-05-18},
	date = {2015},
}

@article{he_spatial_2015,
	title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
	volume = {37},
	doi = {10.1109/TPAMI.2015.2389824},
	pages = {1904--1916},
	number = {9},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2015},
}

@inproceedings{girshick_rich_2014,
	title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {580--587},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	urldate = {2021-05-18},
	date = {2014},
}

@article{wang_scaled-yolov4_2020,
	title = {Scaled-{YOLOv}4: Scaling Cross Stage Partial Network},
	journaltitle = {{arXiv} preprint {arXiv}:2011.08036},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	date = {2020},
}

@software{jocher_ultralyticsyolov5_2021,
	title = {ultralytics/yolov5},
	rights = {{GPL}-3.0 License         ,                 {GPL}-3.0 License},
	url = {https://github.com/ultralytics/yolov5},
	abstract = {{YOLOv}5 in {PyTorch} {\textgreater} {ONNX} {\textgreater} {CoreML} {\textgreater} {TFLite}. Contribute to ultralytics/yolov5 development by creating an account on {GitHub}.},
	publisher = {Ultralytics},
	author = {Jocher, Glenn},
	urldate = {2021-05-09},
	date = {2021-05-09},
	note = {original-date: 2020-05-18T03:45:11Z},
	keywords = {coreml, deep-learning, ios, machine-learning, ml, object-detection, onnx, pytorch, tflite, yolo, yolov3, yolov4, yolov5},
}

@online{noauthor_coco_nodate,
	title = {{COCO} - Common Objects in Context},
	url = {https://cocodataset.org/#detection-eval},
	urldate = {2021-05-09},
}

@inproceedings{lin_microsoft_2014,
	location = {Cham},
	title = {Microsoft {COCO}: Common Objects in Context},
	isbn = {978-3-319-10602-1},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to {PASCAL}, {ImageNet}, and {SUN}. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	pages = {740--755},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	date = {2014},
}

@software{abewley_abewleysort_2021,
	title = {abewley/sort},
	rights = {{GPL}-3.0 License         ,                 {GPL}-3.0 License},
	url = {https://github.com/abewley/sort},
	abstract = {Simple, online, and realtime tracking of multiple objects in a video sequence.},
	author = {abewley},
	urldate = {2021-04-09},
	date = {2021-04-09},
	note = {original-date: 2016-02-03T03:16:23Z},
}

@article{leal-taixe_motchallenge_2015,
	title = {{MOTChallenge} 2015: Towards a Benchmark for Multi-Target Tracking},
	volume = {abs/1504.01942},
	url = {http://arxiv.org/abs/1504.01942},
	journaltitle = {{CoRR}},
	author = {Leal-Taixé, Laura and Milan, Anton and Reid, Ian D. and Roth, Stefan and Schindler, Konrad},
	date = {2015},
	note = {\_eprint: 1504.01942},
}

@online{noauthor_mot_nodate,
	title = {{MOT} Challenge},
	url = {https://motchallenge.net/instructions/},
	urldate = {2021-04-06},
}

@misc{choi_vcm_2020,
	title = {[{VCM}] Object labelled dataset on raw video sequences},
	abstract = {This contribution to the {VCM} activity introduces an object labelled dataset called {SFU}-{HW}-Objects-v1, on raw video sequences, for the case of evaluating both object detection accuracy and video coding efficiency on the same dataset. Object ground-truths for 18 of the {HEVC} v1 {CTC} sequences are labelled, first automatically then manually. The object categories used for the labelling are based on the {COCO} dataset. A total of 21 object classes are found out of the 80 original {COCO} classes. Brief descriptions of the labelling process and initial object detection performance using {YOLOv}3 on the proposed dataset are presented.},
	publisher = {{ISO}/{IEC} {JTC}1/{SC}29/{WG}11 {MPEG}2020/m54737},
	author = {Choi, Hyomin and Hosseini, Elahe and Alvar, Saeed R. and Cohen, Robert A. and Bajić, Ivan V. and Karabutov, Alexander and Zhao, Yin and Alshina, Elena},
	date = {2020-07},
}

@article{sharrab_modeling_2017,
	title = {Modeling and Analysis of Power Consumption in Live Video Streaming Systems},
	volume = {13},
	doi = {10.1145/3115505},
	pages = {11--12},
	journaltitle = {{ACM} Transactions on Multimedia Computing, Communications, and Applications},
	author = {Sharrab, Yousef and Sarhan, Nabil},
	date = {2017},
}

@inproceedings{bachu_review_2015,
	title = {A review on motion estimation in video compression},
	doi = {10.1109/SPACES.2015.7058259},
	abstract = {In modern world video compression technology is developed in to a bloomed field, with several techniques available for a wide range of applications like video transmission, {HDTV}, broadcast digital video. Motion Estimation ({ME}) is a key component for high quality video compression, which is characterized by its high computation complexity and memory requirements. Motion Estimation has been conventionally used in the application of video encoding, but nowadays researchers from various fields other than video encoding are turning towards {ME} to solve various real time problems in their respective fields. The main aim of the survey paper is to analyze {ME} in video compression techniques for video processing, especially to estimate how much amount of data to be compressed, which technique is faster and so on. We also compare video compression techniques with conventional methods like {ES}, {ARP}, Run length, and Huffman coding. The existing conventional techniques will be implemented on the {MATLAB} platform and the performance of video compression technique is evaluated with Compression Ratio ({CR}), Peak Signal to Noise Ratio ({PSNR}) and search patterns.},
	pages = {250--256},
	booktitle = {2015 International Conference on Signal Processing and Communication Engineering Systems},
	author = {Bachu, S. and Chari, K. M.},
	date = {2015-01},
	keywords = {Algorithm design and analysis, {CR}, Databases, Image coding, Motion Estimation ({ME}), Motion estimation, {PSNR}, Prediction algorithms, Video Encoding, Video compression},
}

@article{lou_adaptive_2010,
	title = {Adaptive Motion Search Range Prediction for Video Encoding},
	volume = {20},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2010.2087551},
	abstract = {An adaptive motion search range ({SR}) prediction algorithm for video coding is proposed in this letter. To determine a proper {SR} size for motion vectors ({MVs}) is important to video encoding, since a good choice helps reduce the memory access bandwidth while maintaining the rate-distortion coding performance. To achieve this objective, we first obtain a motion vector predictor ({MVP}) for a target block based on {MVs} of its spatially and temporally neighboring blocks, which form a {MV} prediction set. Then, we relate the variance of the {MV} prediction set to the {SR}. That is, a larger variance implies lower accuracy of the {MVP} and, thus, a larger {SR}. Finally, we derive a probability model for the motion vector prediction difference, the difference between the optimal {MV} and the {MVP}, to quantify the probability for a chosen {SR} to contain the optimal {MV}. The superior performance of the proposed {SR} selection algorithm is demonstrated by experimental results.},
	pages = {1903--1908},
	number = {12},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	author = {Lou, C. and Lee, S. and Kuo, C.- J.},
	date = {2010-12},
	keywords = {Adaptive search range selection, Bandwidth, Encoding, Histograms, Motion control, Noise, Prediction algorithms, Video coding, motion estimation, motion search range, motion vector prediction, video coding},
}

@article{pourazad_hevc_2012,
	title = {{HEVC}: The New Gold Standard for Video Compression: How Does {HEVC} Compare with H.264/{AVC}?},
	volume = {1},
	issn = {2162-2256},
	doi = {10.1109/MCE.2012.2192754},
	abstract = {Digital video has become ubiquitous in our everyday lives; everywhere we look, there are devices that can display, capture, and transmit video. The recent advances in technology have made it possible to capture and display video material with ultrahigh definition ({UHD}) resolution. Now is the time when the current Internet and broadcasting networks do not even have sufficient capacity to transmit large amounts of {HD} content-Let alone {UHD}. The need for an improved transmission system is more pronounced in the mobile sector because of the introduction of lightweight {HD} resolutions (such as 720 pixel) for mobile applications. The limitations of current technologies prompted the International Standards Organization/International Electrotechnical Commission Moving Picture Experts Group ({MPEG}) and International Telecommunication Union-Telecommunication Standardization Sector Video Coding Experts Group ({VCEG}) to establish the Joint Collaborative Team on Video Coding ({JCT}-{VC}), with the objective to develop a new high-performance video coding standard.},
	pages = {36--46},
	number = {3},
	journaltitle = {{IEEE} Consumer Electronics Magazine},
	author = {Pourazad, M. T. and Doutre, C. and Azimi, M. and Nasiopoulos, P.},
	date = {2012-07},
	keywords = {Digital video broadcasting, {HDTV}, Market opportunities, Mobile communication, Performance evaluation, Research and development, Standards, Video coding, Video compression},
}

@article{jacobs_brief_2009,
	title = {A Brief History of Video Coding},
	author = {Jacobs, Marco and Probell, Jonah},
	date = {2009},
}

@article{sullivan_overview_2012,
	title = {Overview of the High Efficiency Video Coding ({HEVC}) Standard},
	volume = {22},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2012.2221191},
	abstract = {High Efficiency Video Coding ({HEVC}) is currently being prepared as the newest video coding standard of the {ITU}-T Video Coding Experts Group and the {ISO}/{IEC} Moving Picture Experts Group. The main goal of the {HEVC} standardization effort is to enable significantly improved compression performance relative to existing standards-in the range of 50\% bit-rate reduction for equal perceptual video quality. This paper provides an overview of the technical features and characteristics of the {HEVC} standard.},
	pages = {1649--1668},
	number = {12},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	author = {Sullivan, G. J. and Ohm, J. and Han, W. and Wiegand, T.},
	date = {2012-12},
	keywords = {Advanced video coding ({AVC}), H.264, High Efficiency Video Coding ({HEVC}), {ISO} standards, Joint Collaborative Team on Video Coding ({JCT}-{VC}), {MPEG} 4 Standard, {MPEG} standards, {MPEG}-4, Moving Picture Experts Group ({MPEG}), Video Coding Experts Group ({VCEG}), Video coding, Video compression, standards, video compression},
}

@article{leal-taixe_tracking_2017,
	title = {Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking},
	volume = {abs/1704.02781},
	url = {http://arxiv.org/abs/1704.02781},
	journaltitle = {{CoRR}},
	author = {Leal-Taixé, Laura and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian D. and Roth, Stefan},
	date = {2017},
	note = {\_eprint: 1704.02781},
}

@software{heindl_cheindpy-motmetrics_2021,
	title = {cheind/py-motmetrics},
	rights = {{MIT} License         ,                 {MIT} License},
	url = {https://github.com/cheind/py-motmetrics},
	abstract = {:bar\_chart: Benchmark multiple object trackers ({MOT}) in Python},
	author = {Heindl, Christoph},
	urldate = {2021-03-25},
	date = {2021-03-24},
	note = {original-date: 2017-04-07T15:16:59Z},
	keywords = {benchmark, clear-mot-metrics, metrics, mot, mot-challenge, object-detection, object-tracking, tracker},
}

@incollection{budagavi_hevc_2014,
	title = {{HEVC} Transform and Quantization},
	isbn = {978-3-319-06894-7},
	pages = {141--169},
	author = {Budagavi, Madhukar and Fuldseth, Arild and Bjøntegaard, Gisle},
	date = {2014},
	doi = {10.1007/978-3-319-06895-4_6},
}

@article{liu_learning_2019,
	title = {Learning Spatial Fusion for Single-Shot Object Detection},
	volume = {abs/1911.09516},
	url = {http://arxiv.org/abs/1911.09516},
	journaltitle = {{CoRR}},
	author = {Liu, Songtao and Huang, Di and Wang, Yunhong},
	date = {2019},
	note = {\_eprint: 1911.09516},
}

@inproceedings{redmon_yolo9000_2017,
	title = {{YOLO}9000: Better, Faster, Stronger},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.html},
	shorttitle = {{YOLO}9000},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {7263--7271},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2021-03-04},
	date = {2017},
}

@software{jocher_ultralyticsyolov3_2021,
	title = {ultralytics/yolov3},
	rights = {{GPL}-3.0 License         ,                 {GPL}-3.0 License},
	url = {https://github.com/ultralytics/yolov3},
	abstract = {{YOLOv}3 in {PyTorch} {\textgreater} {ONNX} {\textgreater} {CoreML} {\textgreater} {TFLite}. Contribute to ultralytics/yolov3 development by creating an account on {GitHub}.},
	publisher = {Ultralytics},
	author = {Jocher, Glenn},
	urldate = {2021-03-03},
	date = {2021-03-03},
	note = {original-date: 2018-08-26T08:57:20Z},
	keywords = {coreml, deep-learning, ios, machine-learning, ml, object-detection, onnx, pytorch, tflite, yolo, yolov3, yolov4, yolov5},
}

@inproceedings{redmon_you_2016,
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	shorttitle = {You Only Look Once},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {779--788},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2021-03-01},
	date = {2016},
}

@article{zou_maize_2020,
	title = {Maize tassels detection: a benchmark of the state of the art},
	volume = {16},
	issn = {1746-4811},
	url = {https://doi.org/10.1186/s13007-020-00651-z},
	doi = {10.1186/s13007-020-00651-z},
	abstract = {The population of plants is a crucial indicator in plant phenotyping and agricultural production, such as growth status monitoring, yield estimation, and grain depot management. To enhance the production efficiency and liberate labor force, many automated counting methods have been proposed, in which computer vision-based approaches show great potentials due to the feasibility of high-throughput processing and low cost. In particular, with the success of deep learning, more and more deeper learning-based approaches are introduced to deal with agriculture automation. Since different detection- and regression-based counting models have distinct characteristics, how to choose an appropriate model given the target task at hand remains unexplored and is important for practitioners.},
	pages = {108},
	number = {1},
	journaltitle = {Plant Methods},
	shortjournal = {Plant Methods},
	author = {Zou, Hongwei and Lu, Hao and Li, Yanan and Liu, Liang and Cao, Zhiguo},
	date = {2020-08-08},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv}3: An Incremental Improvement},
	author = {Redmon, Joseph and Farhadi, Ali},
	date = {2018},
}

@article{zhang_overview_2019,
	title = {An Overview of Emerging Video Coding Standards},
	volume = {22},
	issn = {2375-0529},
	url = {https://doi.org/10.1145/3325867.3325873},
	doi = {10.1145/3325867.3325873},
	abstract = {Today's popular video coding standards, such as H.264/{AVC}, are widely used to encode video into bit streams for storage and transmission. With the explosive growth of various video applications, H.264/{AVC} may not fully satisfy their requirements anymore. There is an increasing demand for high compression efficiency and low complexity video coding standards. In this article, we provide an overview of existing and emerging video coding standards. We review the timeline of the development of the popular H.26X family video coding standards, and introduce several emerging video coding standards such as {AV}1, {VP}9 and {VVC}. As for future video coding, considering the success of machine learning in various fields and hardware acceleration, we conclude this article with a discussion of several future trends in video coding.},
	pages = {13--20},
	number = {4},
	journaltitle = {{GetMobile}: Mobile Comp. and Comm.},
	author = {Zhang, Ticao and Mao, Shiwen},
	date = {2019},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
}

@incollection{sultana_review_2020,
	location = {Singapore},
	title = {A Review of Object Detection Models Based on Convolutional Neural Network},
	isbn = {9789811542886},
	url = {https://doi.org/10.1007/978-981-15-4288-6_1},
	series = {Advances in Intelligent Systems and Computing},
	abstract = {Convolutional neural network ({CNN}) has turned to be the state of the art for object detection task of computer vision. In this chapter, we have reviewed some popular state-of-the-art object detection models based on {CNN}. We have made a categorization of those detection models according to two different approaches: two-stage approach and one-stage approach. Herein, we have explored different gradual developments in two-stage object detection models from R-{CNN} to latest mask R-{CNN} as well as in one-stage detectors from {YOLO} to {RefineDet}. Along with the architectural description of different models, we have focused on training details of each model. We have also drawn a comparison among those models.},
	pages = {1--16},
	booktitle = {Intelligent Computing: Image Processing Based Applications},
	publisher = {Springer},
	author = {Sultana, F. and Sufian, A. and Dutta, P.},
	editor = {Mandal, J. K. and Banerjee, Soumen},
	urldate = {2021-02-20},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-981-15-4288-6_1},
	keywords = {Convolutional neural network, Deep learning, Mask R-{CNN}, Object detection, {RetinaNet}, Review, {YOLO}},
}

@article{lin_detection-free_2016,
	title = {Detection-Free Multiobject Tracking by Reconfigurable Inference With Bundle Representations},
	volume = {46},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2015.2478515},
	abstract = {This paper presents a conceptually simple but effective approach to track multiobject in videos without requiring elaborate supervision (i.e., training object detectors or templates offline). Our framework performs a bi-layer inference of spatio-temporal grouping to exploit rich appearance and motion information in the observed sequence. First, we generate a robust middle-level video representation based on clustered point tracks, namely video bundles. Each bundle encapsulates a chunk of point tracks satisfying both spatial proximity and temporal coherency. Taking the video bundles as vertices, we build a spatio-temporal graph that incorporates both competitive and compatible relations among vertices. The multiobject tracking can be then phrased as a graph partition problem under the Bayesian framework, and we solve it by developing a reconfigurable belief propagation ({BP}) algorithm. This algorithm improves the traditional {BP} method by allowing a converged solution to be reconfigured during optimization, so that the inference can be reactivated once it gets stuck in local minima and thus conduct more reliable results. In the experiments, we demonstrate the superior performances of our approach on the challenging benchmarks compared with other state-of-the-art methods.},
	pages = {2447--2458},
	number = {11},
	journaltitle = {{IEEE} Transactions on Cybernetics},
	author = {Lin, L. and Lu, Y. and Li, C. and Cheng, H. and Zuo, W.},
	date = {2016},
	keywords = {{BP}, Bayesian framework, Graphical inference, Inference algorithms, Partitioning algorithms, Robustness, Target tracking, Trajectory, Videos, belief networks, bilayer inference, bundle representations, clustered point tracks, detection-free multiobject tracking, graph partition problem, graph theory, image motion analysis, image representation, inference mechanisms, motion information, object detection, object tracking, observed sequence, reconfigurable belief propagation algorithm, reconfigurable inference, rich appearance, robust middle-level video representation, spatial proximity, spatio-temporal analysis, spatio-temporal graph, spatio-temporal grouping, temporal coherency, video bundles, video processing, video signal processing},
}

@inproceedings{bewley_simple_2016,
	title = {Simple online and realtime tracking},
	doi = {10.1109/ICIP.2016.7533003},
	abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
	eventtitle = {2016 {IEEE} International Conference on Image Processing ({ICIP})},
	pages = {3464--3468},
	booktitle = {2016 {IEEE} International Conference on Image Processing ({ICIP})},
	author = {Bewley, A. and Ge, Z. and Ott, L. and Ramos, F. and Upcroft, B.},
	date = {2016-09},
	note = {{ISSN}: 2381-8549},
	keywords = {Benchmark testing, Complexity theory, Computer Vision, Data Association, Detection, Detectors, Kalman filters, Multiple Object Tracking, Target tracking, Visualization, detection quality, object detection, object tracking, online tracking, real-time systems, realtime tracking},
}

@inproceedings{li_learning_2009,
	title = {Learning to associate: {HybridBoosted} multi-target tracker for crowded scene},
	doi = {10.1109/CVPR.2009.5206735},
	shorttitle = {Learning to associate},
	abstract = {We propose a learning-based hierarchical approach of multi-target tracking from a single camera by progressively associating detection responses into longer and longer track fragments (tracklets) and finally the desired target trajectories. To define tracklet affinity for association, most previous work relies on heuristically selected parametric models; while our approach is able to automatically select among various features and corresponding non-parametric models, and combine them to maximize the discriminative power on training data by virtue of a {HybridBoost} algorithm. A hybrid loss function is used in this algorithm because the association of tracklet is formulated as a joint problem of ranking and classification: the ranking part aims to rank correct tracklet associations higher than other alternatives; the classification part is responsible to reject wrong associations when no further association should be done. Experiments are carried out by tracking pedestrians in challenging datasets. We compare our approach with state-of-the-art algorithms to show its improvement in terms of tracking accuracy.},
	eventtitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {2953--2960},
	booktitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Li, Y. and Huang, C. and Nevatia, R.},
	date = {2009-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Context modeling, Hybrid intelligent systems, Hybrid power systems, {HybridBoosted} multitarget tracker, Intelligent robots, Layout, Object detection, Parametric statistics, Target tracking, Training data, Trajectory, crowded scene, data association, feature extraction, feature selection, hybrid loss function, image classification, image fusion, learning (artificial intelligence), learning-based hierarchical approach, nonparametric model, object detection, pedestrian tracking, single camera, target tracking, traffic engineering computing},
}

@inproceedings{ristani_performance_2016,
	location = {Cham},
	title = {Performance Measures and a Data Set for Multi-target, Multi-camera Tracking},
	isbn = {978-3-319-48881-3},
	abstract = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080 p, 60 fps video taken by 8 cameras observing more than 2,700 identities over 85 min; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
	pages = {17--35},
	booktitle = {Computer Vision – {ECCV} 2016 Workshops},
	publisher = {Springer International Publishing},
	author = {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
	editor = {Hua, Gang and Jégou, Hervé},
	date = {2016},
}

@article{milan_mot16_2016,
	title = {{MOT}16: A Benchmark for Multi-Object Tracking},
	volume = {abs/1603.00831},
	url = {http://arxiv.org/abs/1603.00831},
	journaltitle = {{CoRR}},
	author = {Milan, Anton and Leal-Taixé, Laura and Reid, Ian D. and Roth, Stefan and Schindler, Konrad},
	date = {2016},
	note = {\_eprint: 1603.00831},
}

@article{luo_multiple_2021,
	title = {Multiple object tracking: A literature review},
	volume = {293},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370220301958},
	doi = {https://doi.org/10.1016/j.artint.2020.103448},
	abstract = {Multiple Object Tracking ({MOT}) has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in an {MOT} system, including formulation, categorization, key principles, evaluation of {MOT} are discussed; 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks; 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative and comprehensive comparisons. By analyzing the results from different perspectives, we have verified some basic agreements in the field; and 4) We provide a discussion about issues of {MOT} research, as well as some interesting directions which will become potential research effort in the future.},
	pages = {103448},
	journaltitle = {Artificial Intelligence},
	author = {Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Kim, Tae-Kyun},
	date = {2021},
	keywords = {Data association, Multi-object tracking, Survey},
}

@article{bernardin_evaluating_2008,
	title = {Evaluating multiple object tracking performance: The {CLEAR} {MOT} metrics},
	volume = {2008},
	doi = {10.1155/2008/246309},
	journaltitle = {{EURASIP} Journal on Image and Video Processing},
	author = {Bernardin, Keni and Stiefelhagen, Rainer},
	date = {2008},
}

@article{ponlatha_comparison_2013,
	title = {Comparison of Video Compression Standards},
	issn = {17938163},
	url = {http://www.ijcee.org/index.php?m=content&c=index&a=show&catid=55&id=854},
	doi = {10.7763/IJCEE.2013.V5.770},
	abstract = {In order to ensure compatibility among video codecs from different manufacturers and applications and to simplify the development of new applications, intensive efforts have been undertaken in recent years to define digital video standards Over the past decades, digital video compression technologies have become an integral part of the way we create, communicate and consume visual information. Digital video communication can be found today in many application sceneries such as broadcast services over satellite and terrestrial channels, digital video storage, wires and wireless conversational services and etc. The data quantity is very large for the digital video and the memory of the storage devices and the bandwidth of the transmission channel are not infinite, so it is not practical for us to store the full digital video without processing. For instance, we have a 720 x 480 pixels per frame,30 frames per second, total 90 minutes full color video, then the full data quantity of this video is about 167.96 G bytes. Thus, several video compression standards, techniques and algorithms had been developed to reduce the data quantity and provide the acceptable quality as possible as can. Thus they often represent an optimal compromise between performance and complexity. This paper describes the main features of video compression standards, discusses the emerging standards and presents some of its main characteristics.},
	pages = {549--554},
	journaltitle = {International Journal of Computer and Electrical Engineering},
	shortjournal = {{IJCEE}},
	author = {Ponlatha, S. and Sabeenian, R. S.},
	urldate = {2021-02-06},
	date = {2013},
	langid = {english},
}

@inproceedings{gavrila_real-time_1999,
	title = {Real-time object detection for "smart" vehicles},
	volume = {1},
	doi = {10.1109/ICCV.1999.791202},
	abstract = {This paper presents an efficient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; efficient hierarchies can be generated offline for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-to-fine approach over the shape hierarchy and over the transformation parameters. Very large speed-up factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of traffic signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardware-specific implementations of the proposed method as far as {SIMD} parallelism is concerned.},
	eventtitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	pages = {87--93 vol.1},
	booktitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	author = {Gavrila, D. M. and Philomin, V.},
	date = {1999-09},
	keywords = {Computer vision, Distance Transforms, Educational institutions, Feature extraction, Image segmentation, Intelligent vehicles, Laboratories, Object detection, Pixel, Shape, Vehicle detection, automotive electronics, computer vision, moving vehicle, object detection, on-board vehicles, pedestrians, real-time vision, shape-based object detection, smart vehicles, template hierarchy, traffic signs},
}

@article{zou_object_2019,
	title = {Object Detection in 20 Years: A Survey},
	url = {http://arxiv.org/abs/1905.05055},
	shorttitle = {Object Detection in 20 Years},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today’s object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century’s time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	journaltitle = {{arXiv}:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2021-02-05},
	date = {2019-05-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhao_overview_2015,
	title = {An overview of object detection and tracking},
	doi = {10.1109/ICInfA.2015.7279299},
	abstract = {Over the last couple of years, object detection and tracking reserachers have been developing many new techniques, which has been used widely by others. In this article, we present an extensive overview of object detection and tracking methods. At the same time, we also introduces some related theoretical knowledge (e.g., feature and classification). The reason why the object detection and tracking in summarized together, is because the object detection can be said to be the foundation of the object tracking, and they all need to choose the right features and training effective classification. Due to the application fields and emphasis may be different, the number of features which we can select is large. This paper mainly introduces some common features, such as color, histogram of gradients edges and optical flow. Then classifications are introduced, which are all classical classifications. There are many methods of detection and tracking, but now researchers will mainly consider some of the key factors, which include context, silhouette and background. Finally, we respectively introduced some common methods for object detection and object tracking. And discuss the advantages and disadvantages of principles.},
	eventtitle = {2015 {IEEE} International Conference on Information and Automation},
	pages = {280--286},
	booktitle = {2015 {IEEE} International Conference on Information and Automation},
	author = {Zhao, Y. and Shi, H. and Chen, X. and Li, X. and Wang, C.},
	date = {2015-08},
	keywords = {Context, Feature extraction, Object detection, Optical imaging, Optical mixing, Support vector machines, Tracking, background, context, detection, feature, gradient edge, image classification, object detection, object detection method, object tracking method, optical flow, tracking},
}