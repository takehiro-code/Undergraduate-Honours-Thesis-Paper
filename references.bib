
@article{leal-taixe_tracking_2017,
	title = {Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking},
	volume = {abs/1704.02781},
	url = {http://arxiv.org/abs/1704.02781},
	journaltitle = {{CoRR}},
	author = {Leal-Taixé, Laura and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian D. and Roth, Stefan},
	date = {2017},
	note = {\_eprint: 1704.02781},
}

@software{heindl_cheindpy-motmetrics_2021,
	title = {cheind/py-motmetrics},
	rights = {{MIT} License         ,                 {MIT} License},
	url = {https://github.com/cheind/py-motmetrics},
	abstract = {:bar\_chart: Benchmark multiple object trackers ({MOT}) in Python},
	author = {Heindl, Christoph},
	urldate = {2021-03-25},
	date = {2021-03-24},
	note = {original-date: 2017-04-07T15:16:59Z},
	keywords = {benchmark, clear-mot-metrics, metrics, mot, mot-challenge, object-detection, object-tracking, tracker},
}

@incollection{budagavi_hevc_2014,
	title = {{HEVC} Transform and Quantization},
	isbn = {978-3-319-06894-7},
	pages = {141--169},
	author = {Budagavi, Madhukar and Fuldseth, Arild and Bjøntegaard, Gisle},
	date = {2014},
	doi = {10.1007/978-3-319-06895-4_6},
}

@article{liu_learning_2019,
	title = {Learning Spatial Fusion for Single-Shot Object Detection},
	volume = {abs/1911.09516},
	url = {http://arxiv.org/abs/1911.09516},
	journaltitle = {{CoRR}},
	author = {Liu, Songtao and Huang, Di and Wang, Yunhong},
	date = {2019},
	note = {\_eprint: 1911.09516},
}

@inproceedings{redmon_yolo9000_2017,
	title = {{YOLO}9000: Better, Faster, Stronger},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.html},
	shorttitle = {{YOLO}9000},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {7263--7271},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2021-03-04},
	date = {2017},
}

@software{jocher_ultralyticsyolov3_2021,
	title = {ultralytics/yolov3},
	rights = {{GPL}-3.0 License         ,                 {GPL}-3.0 License},
	url = {https://github.com/ultralytics/yolov3},
	abstract = {{YOLOv}3 in {PyTorch} {\textgreater} {ONNX} {\textgreater} {CoreML} {\textgreater} {TFLite}. Contribute to ultralytics/yolov3 development by creating an account on {GitHub}.},
	publisher = {Ultralytics},
	author = {Jocher, Glenn},
	urldate = {2021-03-03},
	date = {2021-03-03},
	note = {original-date: 2018-08-26T08:57:20Z},
	keywords = {coreml, deep-learning, ios, machine-learning, ml, object-detection, onnx, pytorch, tflite, yolo, yolov3, yolov4, yolov5},
}

@inproceedings{redmon_you_2016,
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html},
	shorttitle = {You Only Look Once},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {779--788},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2021-03-01},
	date = {2016},
}

@article{zou_maize_2020,
	title = {Maize tassels detection: a benchmark of the state of the art},
	volume = {16},
	issn = {1746-4811},
	url = {https://doi.org/10.1186/s13007-020-00651-z},
	doi = {10.1186/s13007-020-00651-z},
	abstract = {The population of plants is a crucial indicator in plant phenotyping and agricultural production, such as growth status monitoring, yield estimation, and grain depot management. To enhance the production efficiency and liberate labor force, many automated counting methods have been proposed, in which computer vision-based approaches show great potentials due to the feasibility of high-throughput processing and low cost. In particular, with the success of deep learning, more and more deeper learning-based approaches are introduced to deal with agriculture automation. Since different detection- and regression-based counting models have distinct characteristics, how to choose an appropriate model given the target task at hand remains unexplored and is important for practitioners.},
	pages = {108},
	number = {1},
	journaltitle = {Plant Methods},
	shortjournal = {Plant Methods},
	author = {Zou, Hongwei and Lu, Hao and Li, Yanan and Liu, Liang and Cao, Zhiguo},
	date = {2020-08-08},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv}3: An Incremental Improvement},
	author = {Redmon, Joseph and Farhadi, Ali},
	date = {2018},
}

@article{zhang_overview_2019,
	title = {An Overview of Emerging Video Coding Standards},
	volume = {22},
	issn = {2375-0529},
	url = {https://doi.org/10.1145/3325867.3325873},
	doi = {10.1145/3325867.3325873},
	abstract = {Today's popular video coding standards, such as H.264/{AVC}, are widely used to encode video into bit streams for storage and transmission. With the explosive growth of various video applications, H.264/{AVC} may not fully satisfy their requirements anymore. There is an increasing demand for high compression efficiency and low complexity video coding standards. In this article, we provide an overview of existing and emerging video coding standards. We review the timeline of the development of the popular H.26X family video coding standards, and introduce several emerging video coding standards such as {AV}1, {VP}9 and {VVC}. As for future video coding, considering the success of machine learning in various fields and hardware acceleration, we conclude this article with a discussion of several future trends in video coding.},
	pages = {13--20},
	number = {4},
	journaltitle = {{GetMobile}: Mobile Comp. and Comm.},
	author = {Zhang, Ticao and Mao, Shiwen},
	date = {2019},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
}

@article{jacobs_brief_2009,
	title = {A Brief History of Video Coding},
	author = {Jacobs, Marco and Probell, Jonah},
	date = {2009},
}

@incollection{sultana_review_2020,
	location = {Singapore},
	title = {A Review of Object Detection Models Based on Convolutional Neural Network},
	isbn = {9789811542886},
	url = {https://doi.org/10.1007/978-981-15-4288-6_1},
	series = {Advances in Intelligent Systems and Computing},
	abstract = {Convolutional neural network ({CNN}) has turned to be the state of the art for object detection task of computer vision. In this chapter, we have reviewed some popular state-of-the-art object detection models based on {CNN}. We have made a categorization of those detection models according to two different approaches: two-stage approach and one-stage approach. Herein, we have explored different gradual developments in two-stage object detection models from R-{CNN} to latest mask R-{CNN} as well as in one-stage detectors from {YOLO} to {RefineDet}. Along with the architectural description of different models, we have focused on training details of each model. We have also drawn a comparison among those models.},
	pages = {1--16},
	booktitle = {Intelligent Computing: Image Processing Based Applications},
	publisher = {Springer},
	author = {Sultana, F. and Sufian, A. and Dutta, P.},
	editor = {Mandal, J. K. and Banerjee, Soumen},
	urldate = {2021-02-20},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-981-15-4288-6_1},
	keywords = {Convolutional neural network, Deep learning, Mask R-{CNN}, Object detection, {RetinaNet}, Review, {YOLO}},
}

@article{lin_detection-free_2016,
	title = {Detection-Free Multiobject Tracking by Reconfigurable Inference With Bundle Representations},
	volume = {46},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2015.2478515},
	abstract = {This paper presents a conceptually simple but effective approach to track multiobject in videos without requiring elaborate supervision (i.e., training object detectors or templates offline). Our framework performs a bi-layer inference of spatio-temporal grouping to exploit rich appearance and motion information in the observed sequence. First, we generate a robust middle-level video representation based on clustered point tracks, namely video bundles. Each bundle encapsulates a chunk of point tracks satisfying both spatial proximity and temporal coherency. Taking the video bundles as vertices, we build a spatio-temporal graph that incorporates both competitive and compatible relations among vertices. The multiobject tracking can be then phrased as a graph partition problem under the Bayesian framework, and we solve it by developing a reconfigurable belief propagation ({BP}) algorithm. This algorithm improves the traditional {BP} method by allowing a converged solution to be reconfigured during optimization, so that the inference can be reactivated once it gets stuck in local minima and thus conduct more reliable results. In the experiments, we demonstrate the superior performances of our approach on the challenging benchmarks compared with other state-of-the-art methods.},
	pages = {2447--2458},
	number = {11},
	journaltitle = {{IEEE} Transactions on Cybernetics},
	author = {Lin, L. and Lu, Y. and Li, C. and Cheng, H. and Zuo, W.},
	date = {2016},
	keywords = {{BP}, Bayesian framework, Graphical inference, Inference algorithms, Partitioning algorithms, Robustness, Target tracking, Trajectory, Videos, belief networks, bilayer inference, bundle representations, clustered point tracks, detection-free multiobject tracking, graph partition problem, graph theory, image motion analysis, image representation, inference mechanisms, motion information, object detection, object tracking, observed sequence, reconfigurable belief propagation algorithm, reconfigurable inference, rich appearance, robust middle-level video representation, spatial proximity, spatio-temporal analysis, spatio-temporal graph, spatio-temporal grouping, temporal coherency, video bundles, video processing, video signal processing},
}

@inproceedings{bewley_simple_2016,
	title = {Simple online and realtime tracking},
	doi = {10.1109/ICIP.2016.7533003},
	abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9\%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
	eventtitle = {2016 {IEEE} International Conference on Image Processing ({ICIP})},
	pages = {3464--3468},
	booktitle = {2016 {IEEE} International Conference on Image Processing ({ICIP})},
	author = {Bewley, A. and Ge, Z. and Ott, L. and Ramos, F. and Upcroft, B.},
	date = {2016-09},
	note = {{ISSN}: 2381-8549},
	keywords = {Benchmark testing, Complexity theory, Computer Vision, Data Association, Detection, Detectors, Kalman filters, Multiple Object Tracking, Target tracking, Visualization, detection quality, object detection, object tracking, online tracking, real-time systems, realtime tracking},
}

@inproceedings{li_learning_2009,
	title = {Learning to associate: {HybridBoosted} multi-target tracker for crowded scene},
	doi = {10.1109/CVPR.2009.5206735},
	shorttitle = {Learning to associate},
	abstract = {We propose a learning-based hierarchical approach of multi-target tracking from a single camera by progressively associating detection responses into longer and longer track fragments (tracklets) and finally the desired target trajectories. To define tracklet affinity for association, most previous work relies on heuristically selected parametric models; while our approach is able to automatically select among various features and corresponding non-parametric models, and combine them to maximize the discriminative power on training data by virtue of a {HybridBoost} algorithm. A hybrid loss function is used in this algorithm because the association of tracklet is formulated as a joint problem of ranking and classification: the ranking part aims to rank correct tracklet associations higher than other alternatives; the classification part is responsible to reject wrong associations when no further association should be done. Experiments are carried out by tracking pedestrians in challenging datasets. We compare our approach with state-of-the-art algorithms to show its improvement in terms of tracking accuracy.},
	eventtitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {2953--2960},
	booktitle = {2009 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Li, Y. and Huang, C. and Nevatia, R.},
	date = {2009-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Context modeling, Hybrid intelligent systems, Hybrid power systems, {HybridBoosted} multitarget tracker, Intelligent robots, Layout, Object detection, Parametric statistics, Target tracking, Training data, Trajectory, crowded scene, data association, feature extraction, feature selection, hybrid loss function, image classification, image fusion, learning (artificial intelligence), learning-based hierarchical approach, nonparametric model, object detection, pedestrian tracking, single camera, target tracking, traffic engineering computing},
}

@inproceedings{ristani_performance_2016,
	location = {Cham},
	title = {Performance Measures and a Data Set for Multi-target, Multi-camera Tracking},
	isbn = {978-3-319-48881-3},
	abstract = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080 p, 60 fps video taken by 8 cameras observing more than 2,700 identities over 85 min; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
	pages = {17--35},
	booktitle = {Computer Vision – {ECCV} 2016 Workshops},
	publisher = {Springer International Publishing},
	author = {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
	editor = {Hua, Gang and Jégou, Hervé},
	date = {2016},
}

@article{milan_mot16_2016,
	title = {{MOT}16: A Benchmark for Multi-Object Tracking},
	volume = {abs/1603.00831},
	url = {http://arxiv.org/abs/1603.00831},
	journaltitle = {{CoRR}},
	author = {Milan, Anton and Leal-Taixé, Laura and Reid, Ian D. and Roth, Stefan and Schindler, Konrad},
	date = {2016},
	note = {\_eprint: 1603.00831},
}

@article{luo_multiple_2021,
	title = {Multiple object tracking: A literature review},
	volume = {293},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370220301958},
	doi = {https://doi.org/10.1016/j.artint.2020.103448},
	abstract = {Multiple Object Tracking ({MOT}) has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in an {MOT} system, including formulation, categorization, key principles, evaluation of {MOT} are discussed; 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks; 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative and comprehensive comparisons. By analyzing the results from different perspectives, we have verified some basic agreements in the field; and 4) We provide a discussion about issues of {MOT} research, as well as some interesting directions which will become potential research effort in the future.},
	pages = {103448},
	journaltitle = {Artificial Intelligence},
	author = {Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Kim, Tae-Kyun},
	date = {2021},
	keywords = {Data association, Multi-object tracking, Survey},
}

@article{bernardin_evaluating_2008,
	title = {Evaluating multiple object tracking performance: The {CLEAR} {MOT} metrics},
	volume = {2008},
	doi = {10.1155/2008/246309},
	journaltitle = {{EURASIP} Journal on Image and Video Processing},
	author = {Bernardin, Keni and Stiefelhagen, Rainer},
	date = {2008},
}

@article{ponlatha_comparison_2013,
	title = {Comparison of Video Compression Standards},
	issn = {17938163},
	url = {http://www.ijcee.org/index.php?m=content&c=index&a=show&catid=55&id=854},
	doi = {10.7763/IJCEE.2013.V5.770},
	abstract = {In order to ensure compatibility among video codecs from different manufacturers and applications and to simplify the development of new applications, intensive efforts have been undertaken in recent years to define digital video standards Over the past decades, digital video compression technologies have become an integral part of the way we create, communicate and consume visual information. Digital video communication can be found today in many application sceneries such as broadcast services over satellite and terrestrial channels, digital video storage, wires and wireless conversational services and etc. The data quantity is very large for the digital video and the memory of the storage devices and the bandwidth of the transmission channel are not infinite, so it is not practical for us to store the full digital video without processing. For instance, we have a 720 x 480 pixels per frame,30 frames per second, total 90 minutes full color video, then the full data quantity of this video is about 167.96 G bytes. Thus, several video compression standards, techniques and algorithms had been developed to reduce the data quantity and provide the acceptable quality as possible as can. Thus they often represent an optimal compromise between performance and complexity. This paper describes the main features of video compression standards, discusses the emerging standards and presents some of its main characteristics.},
	pages = {549--554},
	journaltitle = {International Journal of Computer and Electrical Engineering},
	shortjournal = {{IJCEE}},
	author = {Ponlatha, S. and Sabeenian, R. S.},
	urldate = {2021-02-06},
	date = {2013},
	langid = {english},
}

@inproceedings{gavrila_real-time_1999,
	title = {Real-time object detection for "smart" vehicles},
	volume = {1},
	doi = {10.1109/ICCV.1999.791202},
	abstract = {This paper presents an efficient shape-based object detection method based on Distance Transforms and describes its use for real-time vision on-board vehicles. The method uses a template hierarchy to capture the variety of object shapes; efficient hierarchies can be generated offline for given shape distributions using stochastic optimization techniques (i.e. simulated annealing). Online, matching involves a simultaneous coarse-to-fine approach over the shape hierarchy and over the transformation parameters. Very large speed-up factors are typically obtained when comparing this approach with the equivalent brute-force formulation; we have measured gains of several orders of magnitudes. We present experimental results on the real-time detection of traffic signs and pedestrians from a moving vehicle. Because of the highly time sensitive nature of these vision tasks, we also discuss some hardware-specific implementations of the proposed method as far as {SIMD} parallelism is concerned.},
	eventtitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	pages = {87--93 vol.1},
	booktitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	author = {Gavrila, D. M. and Philomin, V.},
	date = {1999-09},
	keywords = {Computer vision, Distance Transforms, Educational institutions, Feature extraction, Image segmentation, Intelligent vehicles, Laboratories, Object detection, Pixel, Shape, Vehicle detection, automotive electronics, computer vision, moving vehicle, object detection, on-board vehicles, pedestrians, real-time vision, shape-based object detection, smart vehicles, template hierarchy, traffic signs},
}

@article{zou_object_2019,
	title = {Object Detection in 20 Years: A Survey},
	url = {http://arxiv.org/abs/1905.05055},
	shorttitle = {Object Detection in 20 Years},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today’s object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century’s time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	journaltitle = {{arXiv}:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	urldate = {2021-02-05},
	date = {2019-05-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.05055},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhao_overview_2015,
	title = {An overview of object detection and tracking},
	doi = {10.1109/ICInfA.2015.7279299},
	abstract = {Over the last couple of years, object detection and tracking reserachers have been developing many new techniques, which has been used widely by others. In this article, we present an extensive overview of object detection and tracking methods. At the same time, we also introduces some related theoretical knowledge (e.g., feature and classification). The reason why the object detection and tracking in summarized together, is because the object detection can be said to be the foundation of the object tracking, and they all need to choose the right features and training effective classification. Due to the application fields and emphasis may be different, the number of features which we can select is large. This paper mainly introduces some common features, such as color, histogram of gradients edges and optical flow. Then classifications are introduced, which are all classical classifications. There are many methods of detection and tracking, but now researchers will mainly consider some of the key factors, which include context, silhouette and background. Finally, we respectively introduced some common methods for object detection and object tracking. And discuss the advantages and disadvantages of principles.},
	eventtitle = {2015 {IEEE} International Conference on Information and Automation},
	pages = {280--286},
	booktitle = {2015 {IEEE} International Conference on Information and Automation},
	author = {Zhao, Y. and Shi, H. and Chen, X. and Li, X. and Wang, C.},
	date = {2015-08},
	keywords = {Context, Feature extraction, Object detection, Optical imaging, Optical mixing, Support vector machines, Tracking, background, context, detection, feature, gradient edge, image classification, object detection, object detection method, object tracking method, optical flow, tracking},
}