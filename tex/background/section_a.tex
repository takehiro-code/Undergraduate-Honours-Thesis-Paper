\section{YOLOv3 Object Detector}
\label{sec:background/section_a}

% -------------------------------------------------------
% old text, too much information in YOLO, YOLOv2, YOLOv3 % 
% -------------------------------------------------------

\begin{comment}

As one of the deep learning-based and one-stage object detectors whereby the full image itself is applied to the single neural network, You Only Look Once (YOLO) was considered for our object detection component in the object tracking system. The following conceptual background in YOLO was described by \citeauthor{redmon_you_2016} \cite{redmon_you_2016}. YOLO is a convolutional neural network-based model, and it divides the input image into $S \times S$ grids. Each cell in the grids is responsible for predicting the dimension of $B$ number of bounding boxes. Each bounding box has a center coordinate of $x, y$ and width, and the height of $w, h$ as a dimension, and Intersection over Union (IOU) can be calculated between the dimension of the predicted bounding box and ground truth bounding box. Each cell also predicts the probability of the object contained by the bounding box and the conditional probability of the object belonging to a specific class given the object contained by the bounding box. Based on these three predictions, YOLO gives the class-specific confidence value as,
\begin{equation}
P(class_i | object) \times P(Object) \times IOU = P(class_i) \times IOU,
\label{eq:yolo_conf}
\end{equation}
which is given by \citeauthor{redmon_you_2016}. The dimension of final prediction tensor will be $S \times S \times (B \times 5 + C)$ where $C$ is the number of classes we want to detect and the value 5 contains $x$, $y$, $w$, $h$, and the class-specific confidence score.

YOLO is much faster in speed and suitable to the real-time application but gives relatively lower accuracy than other state-of-the-art two-stage object detectors and hard to localize the small objects. Also, as a limitation, each cell in the grid can predict 2 boxes ($B=2$) but only estimate one class. YOLOv2 has been developed to resolve some issues that exist in YOLO as well as incremental improvements as described by \citeauthor{redmon_yolo9000_2017} \cite{redmon_yolo9000_2017}. YOLOv2 primarily focuses on improving recall and localization while preserving classification errors and speed. Anchor boxes have been implemented to predict the bounding boxes based on the chosen anchor boxes. Using anchor boxes, YOLOv2 estimates a bounding box by an offset from the anchor box instead of estimating the coordinates directly as in YOLO. This allows the network to learn easier and detect multiple objects per cell from multiple anchor boxes. The anchor boxes have been automatically chosen using k-means clustering. Other features are also implemented in YOLOv2, such as batch normalization, high-resolution classifier, and multi-scale training, providing better speed and accuracy. YOLOv3 has been developed as an incremental improvement to the YOLOv2 as described by \citeauthor{redmon_yolov3_2018} \cite{redmon_yolov3_2018}. Similar to YOLOv2, YOLOv3 predicts bounding boxes based on anchor boxes with k-means clustering, and the logistic regression has been implemented to predict the objectiveness score. If the bounding box overlaps on the ground truth more than any other anchor boxes, the objectiveness score will be 1. However, other bounding boxes will be ignored in this cell if they overlap more than than the defined IOU threshold. As other improvements, YOLOv3 uses an independent logistic classifier instead of softmax that is implemented in YOLOv2 to allow multi-label class predictions. Also, YOLOv3 predicts tensor at 3 different scales across the network, and the dimension of the final encoded tensor will be $S \times S \times k \times (5 + C)$ where $k$ is the number of anchor boxes. With these improvements, YOLOv3 achieves localizing smaller objects while maintaining speed and accuracy.

\end{comment}


% -------------------------------------------------------
% re-write here % 
% -------------------------------------------------------

You Only Look Once (YOLO) \cite{redmon_you_2016} was taken into consideration for the choice of our object detection component in the object tracking system. The following conceptual background of YOLO was described by \citeauthor{redmon_you_2016} \cite{redmon_you_2016}. YOLO is a convolutional neural network-based model, and it divides the input image into $S \times S$ grids. Each cell in the grid predicts the bounding box information and the object class probability. YOLO is much faster in speed and more suitable for real-time applications but gives somewhat lower accuracy than other state-of-the-art object detectors. YOLOv2 \cite{redmon_yolo9000_2017} has been developed as an improvement to its predecessor YOLO. YOLOv2 primarily focuses on improving recall and localization while preserving classification accuracy and speed. Anchor boxes have been implemented to improve recall, and YOLOv2 with anchor boxes can predict a lot more bounding boxes than YOLO. YOLOv2 estimates a bounding box by an offset from a candidate anchor box instead of estimating the coordinates directly as in the original YOLO. As an incremental improvement to YOLOv2, YOLOv3 \cite{redmon_yolov3_2018} has been developed. The highlighted improvements are as follows; the logistic regression has been implemented to predict the objectness score. If the bounding box overlaps the ground truth more than any other anchor boxes, the objectness score will be 1. YOLOv3 uses an independent logistic classifier instead of softmax implemented in YOLOv2 to allow multi-label class predictions. Finally, YOLOv3 predicts bounding boxes and the corresponding object classes at 3 different scales across the network, similar to Feature Pyramid Network \cite{lin_feature_2017}. With these improvements, YOLOv3 achieves localization of smaller objects while maintaining speed and accuracy.

We have chosen the YOLOv3 object detector developed by Ultralytics \cite{jocher_ultralyticsyolov3_2021} with the provided pre-trained weights. The developer \citeauthor{jocher_ultralyticsyolov3_2021} pre-trained the network with the backbone of Darknet53, and the model has 261 layers with 61,922,845 parameters. The COCO dataset was used for training with 118,287 training images, 5,000 validation images, and 20,288 out of 40,670 test images. This network with the pre-trained weights can detect up to 80 object classes available from the COCO dataset \cite{lin_microsoft_2014}. To compare the chosen YOLOv3 detector from Ultralytics with the original implementation of YOLOv3 by \citeauthor{redmon_yolov3_2018}, the comparison of Mean Average Precision (mAP) on the COCO dataset has been shown in Table \ref{tab:yolov3_comparison}. The mAP value from the original implementation of Darknet53 is reported by the author \citeauthor{redmon_yolov3_2018}, and the value from the Ultralytics implementation is reported by \citeauthor{jocher_ultralyticsyolov3_2021} \cite{jocher_ultralyticsyolov3_2021}. mAP is a detection performance metric, and it averages Average Precision (AP) values from different object classes. In the COCO Detection Challenge, mAP also averages AP values at 10 intersection over union (IOU) thresholds from 0.5 to 0.95 \cite{lin_microsoft_2014} \cite{noauthor_coco_nodate}. mAP-50 is a detection performance metric at IOU threshold of 0.5 in particular, and its detail definition is explained in Section \ref{sec:background/section_d}. The mAP and mAP-50 are reported based on Microsoft COCO dataset. As Table \ref{tab:yolov3_comparison} shows, our chosen detector from Ultralytics achieves better detection performance than the original implementation of Darknet53.
\input{tex/tables/YOLOv3_comparison}