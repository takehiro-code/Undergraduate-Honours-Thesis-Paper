\section{YOLOv3 Object Detector}
\label{sec:background/section_a}

% -------------------------------------------------------
% old text, too much information in YOLO, YOLOv2, YOLOv3 % 
% -------------------------------------------------------

\begin{comment}

As one of the deep learning-based and one-stage object detectors whereby the full image itself is applied to the single neural network, You Only Look Once (YOLO) was considered for our object detection component in the object tracking system. The following conceptual background in YOLO was described by \citeauthor{redmon_you_2016} \cite{redmon_you_2016}. YOLO is a convolutional neural network-based model, and it divides the input image into $S \times S$ grids. Each cell in the grids is responsible for predicting the dimension of $B$ number of bounding boxes. Each bounding box has a center coordinate of $x, y$ and width, and the height of $w, h$ as a dimension, and Intersection over Union (IOU) can be calculated between the dimension of the predicted bounding box and ground truth bounding box. Each cell also predicts the probability of the object contained by the bounding box and the conditional probability of the object belonging to a specific class given the object contained by the bounding box. Based on these three predictions, YOLO gives the class-specific confidence value as,
\begin{equation}
P(class_i | object) \times P(Object) \times IOU = P(class_i) \times IOU,
\label{eq:yolo_conf}
\end{equation}
which is given by \citeauthor{redmon_you_2016}. The dimension of final prediction tensor will be $S \times S \times (B \times 5 + C)$ where $C$ is the number of classes we want to detect and the value 5 contains $x$, $y$, $w$, $h$, and the class-specific confidence score.

YOLO is much faster in speed and suitable to the real-time application but gives relatively lower accuracy than other state-of-the-art two-stage object detectors and hard to localize the small objects. Also, as a limitation, each cell in the grid can predict 2 boxes ($B=2$) but only estimate one class. YOLOv2 has been developed to resolve some issues that exist in YOLO as well as incremental improvements as described by \citeauthor{redmon_yolo9000_2017} \cite{redmon_yolo9000_2017}. YOLOv2 primarily focuses on improving recall and localization while preserving classification errors and speed. Anchor boxes have been implemented to predict the bounding boxes based on the chosen anchor boxes. Using anchor boxes, YOLOv2 estimates a bounding box by an offset from the anchor box instead of estimating the coordinates directly as in YOLO. This allows the network to learn easier and detect multiple objects per cell from multiple anchor boxes. The anchor boxes have been automatically chosen using k-means clustering. Other features are also implemented in YOLOv2, such as batch normalization, high-resolution classifier, and multi-scale training, providing better speed and accuracy. YOLOv3 has been developed as an incremental improvement to the YOLOv2 as described by \citeauthor{redmon_yolov3_2018} \cite{redmon_yolov3_2018}. Similar to YOLOv2, YOLOv3 predicts bounding boxes based on anchor boxes with k-means clustering, and the logistic regression has been implemented to predict the objectiveness score. If the bounding box overlaps on the ground truth more than any other anchor boxes, the objectiveness score will be 1. However, other bounding boxes will be ignored in this cell if they overlap more than than the defined IOU threshold. As other improvements, YOLOv3 uses an independent logistic classifier instead of softmax that is implemented in YOLOv2 to allow multi-label class predictions. Also, YOLOv3 predicts tensor at 3 different scales across the network, and the dimension of the final encoded tensor will be $S \times S \times k \times (5 + C)$ where $k$ is the number of anchor boxes. With these improvements, YOLOv3 achieves localizing smaller objects while maintaining speed and accuracy.

\end{comment}


% -------------------------------------------------------
% re-write here % 
% -------------------------------------------------------

As one of the deep learning-based and one-stage object detectors whereby the full image itself is applied to the single neural network, You Only Look Once (YOLO) was considered for our object detection component in the object tracking system. The following conceptual background in YOLO was described by \citeauthor{redmon_you_2016} \cite{redmon_you_2016}. YOLO is a convolutional neural network-based model, and it divides the input image into $S \times S$ grids. Each cell in the grid predicts the bounding box information and the object class probability. YOLO is much faster in speed and suitable to the real-time application but gives relatively lower accuracy than other state-of-the-art two-stage object detectors and hard to localize the small objects. YOLOv2 has been developed as an improvement to the predecessor of YOLO. YOLOv2 primarily focuses on improving recall and localization while preserving classification errors and speed \cite{redmon_yolo9000_2017}. Anchor boxes have been implemented to predict the bounding boxes based on the selected anchor boxes by k-means clustering according to \citeauthor{redmon_yolo9000_2017}. Using anchor boxes, YOLOv2 estimates a bounding box by an offset from an anchor box instead of estimating the coordinates directly as in YOLO. As an incremental improvement to YOLOv2, YOLOv3 has been developed. The highlighted improvements are as follows \cite{redmon_yolov3_2018}; the logistic regression has been implemented to predict the objectiveness score. If the bounding box overlaps the ground truth more than any other anchor boxes, the objectiveness score will be 1. YOLOv3 uses an independent logistic classifier instead of softmax implemented in YOLOv2 to allow multi-label class predictions. Finally, YOLOv3 predicts a tensor at 3 different scales across the network, similar to Feature Pyramid Network. With these improvements, YOLOv3 achieves localizing smaller objects while maintaining speed and accuracy.


We have chosen the YOLOv3 object detector developed by Ultralytics \cite{jocher_ultralyticsyolov3_2021} with the provided pre-trained weight. The developer \citeauthor{jocher_ultralyticsyolov3_2021} pre-trained the network with the backbone of darknet53, and the model is 261 layers with 61922845 parameters. The 2017 COCO dataset was used for training with 118287 train images, 5000 validation images, and 20288 out of 40670 test images. This network with the pre-trained weight can detect objects up to 80 object classes available from the COCO dataset. To compare the chosen YOLOv3 detector from Ultralytics with the original implementation of YOLOv3 by \citeauthor{redmon_yolov3_2018}, the comparison of Average Precision on the COCO dataset has been shown in Table \ref{tab:yolov3_comparison}.
\input{tex/tables/YOLOv3_comparison}
mAP is "mean average precision" but also denoted as "AP". mAP averages the average precision values at 10 IOU threshold from 0.5 to 0.95. For AP\textsubscript{50} is the average precision value at IOU threshold of 0.5 \cite{noauthor_coco_nodate}. The assumption on this comparison is that the AP and AP\textsubscript{50} are reported based on the 2017 version of the Microsoft COCO dataset. As Table \ref{tab:yolov3_comparison} shows, our chosen detector from Ultralytics achieves a better detection performance than the original implementation of Darknet53.