\section{Multiple Object Tracking Metrics}
\label{sec:background/section_d}

To evaluate the object detection and tracking performance quantitatively, we need metrics to quantify the performance with respect to the ground truth. Since there is no single metric that captures all aspects of the tracking performance, various metrics have been considered as listed below. The arrow symbol $\uparrow$ indicates "the higher, the better", while $\downarrow$ indicates "the lower, the better". We utilized the software from \cite{heindl_cheindpy-motmetrics_2021} to evaluate the performance metrics.

\subsection{Detection Performance Measure}
The following metrics of TP, FP, FN, Precision, Recall, F1, and mAP-50 measure the detection performance. TP, FP, and FN are based on the value of Intersection over Union (IOU) threshold, which is defined as the area of intersection of the detected bounding box and the ground truth bounding box, divided by the union of those boxes. For example, if we set the IOU threshold to 0.5 and obtain the IOU value of 0.8 at the target object, we count it as TP. If we obtain the IOU value of 0.2, we then count it as FP. If there is no detected bounding at the ground truth target, FN will be counted. The following definitions of detection performance measure are explained in \cite{ristani_performance_2016} \cite{milan_mot16_2016}. For mAP-50, we explained the definition adapted from \cite{padilla_survey_2020} and \cite{padilla_comparative_2021}.

\begin{itemize}

\item \textbf{TP ($\uparrow$)}: True Positive. The number of times the detector correctly detects a target. 

\item \textbf{FP ($\downarrow$)}: False Positive. The number of times the detector falsely detects a target.

\item \textbf{FN ($\downarrow$)}: False Negative. This metric is the opposite of FP, i.e. the number of times the detector fails to detect a target.

\item \textbf{Precision ($\uparrow$)}: The number of correct detection divided by the number of all detection made by the detector.
\begin{equation}
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\label{eqn:Precision}
\end{equation}

\item \textbf{Recall ($\uparrow$)}: The number of correct detections divided by the number of objects from the ground truth.
\begin{equation}
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\label{eqn:Recall}
\end{equation}

\item \textbf{F1 ($\uparrow$)}: F1 assesses the detection performance as the harmonic mean of precision and recall.
\begin{equation}
\text{F1} = 2\frac{\text{TP} \cdot \text{FN}}{\text{TP} + \text{FN}}
\label{eqn:F1}
\end{equation}

\item \textbf{mAP-50 ($\uparrow$)}: Mean Average Precision (mAP) is the metric that assesses the detection performance and is the most popular metric used in the benchmarks \cite{padilla_comparative_2021}. mAP is the mean of Average Precision (AP) value for each object class. We adopted the mAP metric that evaluates the detection performance at IOU threshold of 0.5. We call it as mAP-50. mAP evaluated at IOU threshold of 0.5 is the metric based on the PASCAL VOC challenge \cite{everingham_pascal_2015}. What mAP-50 differs from the metric F1 is that it evaluates the detection performance at multiple confidence thresholds from the detector while F1 evaluates only at one specific confidence threshold. With the given confidence threshold, the detector will only detect the objects with the object class probability greater than the threshold. To quantify the detection performance that accounts for different confidence thresholds, we would like to calculate the integration of Precision and Recall over different confidence thresholds, which will be the area under the curve of $\text{Precision}(s) \times \text{Recall}(s)$. This area under the curve will be the AP value. Note that Precision and Recall are now subject to the different confidence threshold $s$. However, the curve is often zig-zag, so the interpolation of Precision at different Recall will be necessary. By doing the interpolation, AP can be obtained via Riemann sums as following equations adapted from \cite{padilla_survey_2020} \cite{padilla_comparative_2021}. 
\begin{equation}
\text{AP} = \sum_{n} \left( R_{n+1} - R_{n} \right) P_{\text{interp}} \left( R_{n+1}) \right)
\label{eqn:AP}
\end{equation}
\begin{equation}
P_{\text{interp}} \left( R_{n+1} \right) = \max_{\substack{R \left(s \right) \geq R_{n}}} P \left( R \left(s \right) \right)
\end{equation}
\label{eqn:P_interp}
We call Precision and Recall as $P$ and $R$ respectively for simplicity in the equations. $P_{\text{interp}}$ is the interpolated Precision which takes the maximum Precision at given all available Recall values greater than or equal to $R_{n}$. $R_{n}$ is the previous interpolated Recall value and $R_{n+1}$ is the current interpolated Recall value. Based on this definition, AP can be computed iteratively and $n$ indicates $n$-th iteration. This interpolation method is called all-point interpolation \cite{padilla_survey_2020}. Evaluating AP value for each object class at IOU threshold of 0.5 and averaging them with the total number of object classes, we obtain mAP-50 as follows.
\begin{equation}
\text{mAP-50} = \frac{1}{C} \sum_{i=1}^{C} \left( \text{AP-50} \right)_i
\label{eqn:mAP}
\end{equation}
where $C$ is the number of object classes and $\left( \text{AP-50} \right)_i$ is the AP value at $i$-th class with the IOU threshold of 0.5. We utilized the software \cite{cartucho_cartuchomap_2021} to compute mAP-50 using all-point interpolation and their corresponding literature is \cite{cartucho_robust_2018}.


\end{itemize}


\subsection{ID Measure}
IDP, IDR, IDF1 are measures of the tracker's ability to identify object trajectories. IDTP, IDFP, and IDFN are used in the definitions of IDP, IDR, and IDF1. \citeauthor{ristani_performance_2016} \cite{ristani_performance_2016} listed the following definitions of ID performance measures.

\begin{itemize}

\item \textbf{IDTP ($\uparrow$)}: The number of correct identifications of the trajectories.

\item \textbf{IDFP ($\downarrow$)}: The number of incorrect identifications of the trajectories.

\item \textbf{IDFN ($\downarrow$)}: The number of times the tracker fails to identify the true trajectories.

\item \textbf{IDP ($\uparrow$)}: Identification Precision. Similar to the definition of Precision, the number of correct identifications is divided by the number of all identifications.
\begin{equation}
\text{IDP} = \frac{\text{IDTP}}{\text{IDTP} + \text{IDFP}}
\label{eqn:IDP}
\end{equation}

\item \textbf{IDR ($\uparrow$)}: Identification Recall. The number of correct identifications is divided by the number of ground truth identifications.
\begin{equation}
\text{IDR} = \frac{\text{IDTP}}{\text{IDTP} + \text{IDFN}}
\label{eqn:IDR}
\end{equation}

\item \textbf{IDF1 ($\uparrow$)}: IDF1 assesses the identification performance as the harmonic mean of IDP and IDR. 
\begin{equation}
\text{IDF1} = 2 \cdot \frac{\text{IDP} \cdot \text{IDR}}{\text{IDTP} + \text{IDFN}}
\label{eqn:IDF1}
\end{equation}

\item \textbf{IDs ($\downarrow$)}: Identity switch counts the number of times a different identity is assigned to a matched trajectory. This is also called a mismatch. IDs is only counted when the newly assigned identity $\textit{i}$ on the trajectory matches with the ground truth trajectory with the identity $\textit{k}$, and the previously assigned trajectory with the identity $\textit{j}$ is not the same as $\textit{i}$ \cite{milan_mot16_2016}. This metric is also sometimes denoted as IDSW in the literature \cite{milan_mot16_2016}.
\end{itemize}



\subsection{Track Quality Measure}
MT, PT, ML, and FM are metrics that measure the track quality. Note that these metrics do not account for ID assignments, so ID does not have to be the same on the trajectory. The following definitions of track quality metrics are explained by \cite{milan_mot16_2016}.

\begin{itemize}

\item \textbf{GT}: It counts the number of ground truth trajectories.

\item \textbf{MT ($\uparrow$)}: Mostly Tracked. It counts the number of trajectories where each trajectory is being tracked at least 80\% of the time with respect to the entire time of the ground truth trajectory.

\item \textbf{PT ($\downarrow$)}: Partially Tracked. It counts the number of trajectories where each trajectory is being tracked at least 20\% of the time but less than 80\% with respect to the entire time of the ground truth trajectory.

\item \textbf{ML ($\downarrow$)}: Mostly Loss. It counts the number of trajectories where each trajectory is being tracked less than 20\% of the time with respect to the entire time of the ground truth trajectory.

\item \textbf{FM ($\downarrow$)}: Fragmentation counts the number of times the trajectories being tracked switched to untracked.
\end{itemize}



\subsection{CLEAR MOT Metrics}
According to \cite{bernardin_evaluating_2008}, classification of events, activities, and relationships (CLEAR) evaluations introduced two metrics of MOTA and MOTP in 2008.
Although there is no single metric that is able to assess the all aspects of tracking performance, MOTA and MOTP emphasize the more general and overall performance of the tracker. Especially, MOTA is considered the most popular metric to assess the overall tracking performance \cite{milan_mot16_2016} and according to \cite{leal-taixe_tracking_2017}, MOTA is the best measure that aligns with the human visual assessment of tracking accuracy.

\begin{itemize}
\item \textbf{MOTA ($\uparrow$)}: Multiple Object Tracking Accuracy. MOTA is a metric that combines three other metrics: FN, FP, and IDs. It scores the overall tracking performance in the context of MOT and the value could range in $\left( -\infty, 100 \right]$. The score becomes negative when the overall error count exceeds the total number of ground truth trajectories.
\begin{equation}
\text{MOTA} = 1 - \frac{\sum_{t} (\text{FN}_{t} + \text{FP}_{t} + \text{IDs}_{t})}{\sum_{t}\text{GT}_{t}}
\label{eqn:MOTA}
\end{equation}
where $t$ is the frame index.

\item \textbf{MOTP ($\uparrow$ in \%)}: Multiple Object Tracking Precision. The numerator shows the total overlap of all target bounding boxes with respect to the ground truth bounding boxes for all the frames, and the denominator indicates the total number of matches for all the frames. This measure quantifies how well the objects are localized by the detector in MOT, but it does not give a good indication of the overall tracking performance.
\begin{equation} 
\text{MOTP} = \frac{\sum_{t,i} d_{t,i}}{\sum_{t}c_{t}}
\label{eqn:MOTP}
\end{equation}
where $d_{t,i}$ shows how much a target bounding box overlaps with the ground truth bounding box at target $i$ at a frame index $t$ while $c_{t}$ denotes the matched objects with the correct ID at a frame index $t$. We report this score as a percentage rather than the value based on the original definition; the following conversion is made.
\begin{equation} 
\text{MOTP} (\%) = (1 - \text{MOTP}) \cdot 100
\label{eqn:MOTP_percentage}
\end{equation}

\end{itemize}

