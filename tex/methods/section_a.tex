\section{Description of Datasets}
\label{sec:methods/section_a}

To our knowledge, there is no public or open-source dataset of uncompressed video sequences with object tracking ground truth. Our group members prepared the uncompressed video sequences and annotated them to obtain object detection ground truth, using YOLOv3 \cite{redmon_yolov3_2018} and YOLO Mark Software \cite{alexey_alexeyabyolo_mark_2021} as the semi-automated annotation process \cite{choi_vcm_2020}. The existing annotations prepared by our group members are suitable for object detection; however, for the purpose of analyzing the tracking performance, we further annotated the unique object identifier (ID) on the existing ground truths using Normalized Cross-correlation (NCC) \cite{zhao_image_2006}. NCC value gives the similarity between the two images. Figure \ref{fig:annotation} shows the semi-automated annotation procedure for assigning the unique IDs.
\input{tex/figures/annotation}
Given the existing ground truth, each object annotation is in the YOLO format as [Class ID, Center x, Center y, Width, Height, Confidence]. Class ID indicates the identifier to the type of object class; for example, "person" as 0, "sports ball" as 32, and "chair" as 56, which are part of the 80 COCO object classes \cite{lin_microsoft_2014}. Center x and Center y are the center position of the bounding box; Width and Height are the corresponding dimensions of boxes. We compared every object in the previous frame $n$ with the current frame $n+1$ and computed NCC values for every possible pair between those image patches of objects. We take the pair of objects that corresponds to the highest NCC value, and if the value is greater than or equal to the threshold, we assign an unique ID automatically but otherwise we manually assign an ID. Threshold is a value that tells how confident the two image patches of objects are similar, and we chose 0.60 for most video sequences. Throughout this semi-automated process of annotation, object IDs are assigned in the second column of ground truth files.
\input{tex/tables/seq_list}
Table \ref{tab:seq_list} shows the 13 uncompressed video sequences for which we created ground truth annotations, out of 18 available video sequences from \cite{choi_vcm_2020}. The sequence class (B, C, D, E) indicates the resolution ($\text{Width} \times \text{Height}$). Each sequence has different number of object classes and each class ID is from the 80 COCO object classes \cite{lin_microsoft_2014}. Table \ref{tab:class_id}, adapted from \cite{choi_vcm_2020}, shows the corresponding object class name for each class ID, and we only listed the object classes that we detect and track in the given sequences from Table \ref{tab:seq_list}.
\input{tex/tables/class_id}
In the experiments, "all" refers to all object classes available in the ground truth. The uncompressed sequences are in the YUV420 format. The object tracking pipeline consists of the YOLOv3 detector and SORT, as shown in Figure \ref{fig:yolov3+SORT}. 
\input{tex/figures/YOLOv3+SORT}
Input video sequences of PNG files are input to the YOLOv3 object detector. The output from YOLOv3 will be generated in the YOLO format as [Class ID, Center x, Center y, Width, Height, Confidence]. Note that Confidence is the object class probability, and this score tells how confident the object is detected for the particular class. These results are converted to the MOT format used in the \textit{MOTChallenge} 2015 benchmark \cite{leal-taixe_motchallenge_2015}. The object ID, the unique identifier to the object, for the detected result is initialized as -1. Applying SORT to this detected result, we obtain the tracking result in the MOT format with the assigned object ID as [Frame index, Object ID, Top-left x, Top-left y, Width, Height, Confidence, 3D x, 3D y, 3D z]. Top-left x and Top-left y are the positions of the bounding box at the top-left corner. 3D x, 3D y, 3D z is the bounding box position in 3D, but we assigned -1 in our experiment since the 3D position is not applicable to our experiment. The ground truth is also converted from the YOLO format to the MOT format.

% The MOT format from \textit{MOTChallenge} is summarized in 
% \begin{myfont}
% \centering
% Class ID, Object ID, center X, center Y, Width, Height
% \end{myfont}


% \begin{table}[]
%     \centering
%     \caption{}
%     \begin{tabular}{|c|c|}
%         \hline
%         Data field & Description \\
%         \hline\hline
%         Frame Index & Frame index in a sequence \\
%         \hline
%         Object ID & Unique identifier to the object \\
%         \hline
%         Top-left x & x coordinate in top-left corner of the bounding box \\
%         \hline
%         Top-left y & y coordinate in top-left corner of the bounding box \\
%         \hline
%         Width & Width of the bounding box \\
%         \hline
%         Height & Height of the bounding box \\
%         \hline
%         Confidence & Confidence score of the detection of the object \\
%         \hline
%         3D x & x coordinate in 3D bounding box \\
%         \hline
%         3D y & y coordinate in 3D bounding box \\
%         \hline
%         3D z & z coordinate in 3D bounding box \\
%         \hline
%     \end{tabular}
%     \label{tab:MOT_format}
% \end{table}


